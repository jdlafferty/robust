% !TEX root = ./robust.tex

\section{Literature Review}

Early work on robust regression with noise corrupted by oblivious adversary includes \cite{wright2010dense,nguyen2013exact,nguyen2012robust}. These results allow the contamination proportion $\epsilon$ tend to one, but consistency of the regression coefficients was not established in the literature until \cite{tsakonas2014convergence}. This work showed that it is possible to achieve consistency in robust regression even when $\epsilon\rightarrow 1$ via $\ell_1$-minimization. However, the sample complexity required by \cite{tsakonas2014convergence} is an exponential function of dimension.

Recently, the paper \cite{suggala2019adaptive} considered an iterative thresholding procedure that is based on the ideas of \cite{jain2014iterative,bhatia2017consistent}, and established consistency when $\epsilon\rightarrow 1$ with nearly optimal sample complexity. However, a careful study of the proof reveals that the result of \cite{suggala2019adaptive} requires the contaminated data to be bounded, and therefore arbitrary contamination is not allowed. More significantly, the iterative thresholding procedure relies on the knowledge of noise variance. This is problematic because the noise variance is a quantity that is not identifiable in the setting of robust regression with arbitrary contamination. This rules out the possibility of a plug-in noise variance estimator and makes their procedure impractical. Interestingly, \cite{suggala2019adaptive} can also handle heavy-tailed noise distributions, including Cauchy distributions. It is not clear whether their procedure is robust to heavy-tailed distributions and arbitrary contamination simultaneously.

To the best of our knowledge, the only result that deals with arbitrary contamination and shows consistency with nearly optimal sample complexity when $\epsilon\rightarrow 1$ is by \cite{gao2020model}. However, the result of \cite{gao2020model} is only derived in a noiseless setting. In the current paper, we generalize \cite{gao2020model} to general heavy-tailed noise distributions and also consider sparsity in the regression coefficients. In particular, we study  the simple penalized $\ell_1$-minimization procedure, and establish its robustness to arbitrary contamination, heavy-tailed noise, and heavy-tailed design distributions simultaneously.

The penalized $\ell_1$-minimization procedure, also known as the penalized median regression, is a classical method that has been well studied in the statistics literature. But we emphasize that the existing statistics literature only treats heavy-tailed distributions without contamination, that is, it assumes $\epsilon=0$. Optimal error rates in this special setting of robust sparse regression have been obtained by \cite{wang2013l1}. More generally, sparse quantile regression was studied by \cite{belloni2011l1}, and  sparse Huber regression was analyzed in \cite{loh2017statistical,sun2020adaptive}. Both quantile regression and Huber regression include median regression as a special case.

Finally, we discuss a very different contamination model of regression that has also been considered in the literature. This setting allows both the design matrix and the response vector to be contaminated. To be specific, consider $(x_i,y_i)\in\mathbb{R}^p\times\mathbb{R}$ for $i\in[n]$. For each $i\in[n]$, $(x_i,y_i)\sim (1-\epsilon)P_{\beta}+\epsilon Q$ independently, where $P_{\beta}$ can be a Gaussian linear model with a (sparse) regression vector $\beta\in\mathbb{R}^p$, and $Q$ is an arbitrary contamination distribution. We refer the readers to \cite{chen2013robust,balakrishnan2017computationally,diakonikolas2019efficient,liu2020high} and references therein. In particular, it is shown by \cite{chen2018robust} and \cite{gao2020robust} that the minimax rate of estimating $\beta$ with respect to the squared $\ell_2$ loss is $\frac{s\log p}{n}+\epsilon^2$. Thus, in order to achieve consistency, it is necessary that $\epsilon\rightarrow 0$.
This is very different from the setting studied in the current paper, where the design matrix is not contaminated. With a \textit{clean} design matrix, the rate we obtain is $\frac{s\log p}{n(1-\epsilon)^2}$, which allows consistency even when $\epsilon\rightarrow 1$.

Another closely related contamination model is where only the response vector or the noise vector is contaminated, and the contamination is not only arbitrary, but is also allowed to depend on the design matrix. This is known as the \textit{adaptive} adversary in the literature, in contrast to the \textit{oblivious} adversary setting considered in our paper. Information-theoretically, the adaptive adversary setting is equivalent to $(x_i,y_i)\sim (1-\epsilon)P_{\beta}+\epsilon Q$, since the minimax rate is also $\frac{s\log p}{n}+\epsilon^2$. However, it is interesting to note that the rate $\frac{s\log p}{n}+\epsilon^2$ in the adaptive adversary setting can be achieved by very simple procedures such as penalized M-estimators \citep{dalalyan2019outlier}.
