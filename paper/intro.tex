% !TEX root = ./robust.tex

\section{Introduction}
\label{sec:intro}

In this paper we study robust regression in the setting where the noise has outliers or contaminated values,
and the model parameters are estimated by minimizing the residual sum of absolute values. We adopt the standard regression model $y_i=x_i^T\beta^*+z_i$ for $n$ data points $(x_i,y_i)$, but assume that the noise is distributed according to a mixture, with $z_i\sim (1-\epsilon)P_i+\epsilon Q_i$ independently. The default noise distribution $P_i$ is assumed to have ``nice'' properties, but the corrupting distribution
$Q_i$ is completely arbitrary. The parameter $\epsilon$ indicates the fraction of the observations $y_i$ that are corrupted. The quantities $\epsilon$, $P_i$ and $Q_i$ are unknown; only the points $(x_1, y_1), \ldots, (x_n, y_n)$ are observed.



When the data are high dimensional, we add an $\ell_1$ penalty to the objective function, resulting in the estimator
\begin{equation}
  \wh{\beta}=\argmin_{\beta\in\mathbb{R}^p}\left[\frac{1}{n}\|y-X\beta\|_1+\lambda\|\beta\|_1\right].
  \label{robustlasso}
\end{equation}
This optimization is equivalent to a linear program, and can be thought of as a robust Lasso estimator. The main result in this paper implies that the error of this estimator scales as
\begin{equation}
  \|\hat \beta - \beta^*\|  = O_P\left(\frac{1}{1-\epsilon} \sqrt{\frac{s\log(p)}{n}}\right),
\end{equation}
under appropriate assumptions.
Here $s = \|\beta^*\|_0$ is the number of nonzero coefficients in the true model $\beta^*$, $n$ is the sample size, and $p$ is the number of predictor variables. The assumptions and precise statement of this result are given in Section~\ref{sec:main}. The primary aspect of this scaling that we highlight here is the factor of $1/(1-\epsilon)$, which implies that even for a large proportion $\epsilon \to 1$ of corrupted values, the model is consistently estimated by the robust lasso as the sample size increases. Figure~2 illustrates this scaling behavior with respect to $\epsilon, n, p$ in simulation.

In our analysis of the estimator \eqref{robustlasso}, it is seen that the optimal regularization parameter does not depend on the noise distributions $P_i$. In other words, the
estimator is pivotal with respect to the parameters of the noise, viewed as nuisance parameters.
In contrast, algorithms such as the lasso and iterative thresholding  require knowledge of the noise variance \citep{bickel2009simultaneous,suggala2019adaptive}.
The pivotal property is particularly important in the robust setting, where the
noise variance is not identifiable; we discuss this point further in Section~\ref{sec:main}.
In addition, we show how the robust lasso can be effective when the design variables $X_j$ and the noise distributions $P_i$ have heavy tails.
In fact, this procedure is robust to arbitrary contamination, heavy-tailed noise, and heavy-tailed design distributions simultaneously. Algorithmically, we show how the linear program \eqref{robustlasso} can be reformulated as median regression with respect to a transformed design matrix.

In the following section we review the existing literature on the problem of robust regression for corrupted noise, which has seen significant attention in recent years. We also relate this problem to previous work that considers corrupted design and noise together. The main results of the paper are presented in Section~\ref{sec:main}, where we establish the scaling behavior of median regression under different assumptions. We separate the cases where the design variables are assumed to have sub-Gaussian and heavy tails. A series of simulations that exhibit close correspondence with this theory are presented in Section~\ref{sec:experiments}. The detailed proofs of these results are collected in Section~\ref{sec:proofs}.  Finally, we discuss these results and their implications for applications in Section~\ref{sec:discuss}. We also mention directions for future research, including extensions to robust quantile regression.

% WLOG assumption on median zero
